# Recurrent Neural Network(RNN): LSTM

- ```LSTM```은 Long Short Term Memory의 약자로 최근 RNN에선 Standard로 사용되고 있음
- Recurrent : 이전에서 어떤 정보가 추가적으로 왔음
- 마치 메모리가 하나 있는 것처럼 이전에 까지 온 데이터를 취합한 메모리 + 현재 입력을 같이 고려하는 구조

<img src="../images/rnn001.png" height="250">

### Long-Term Dependencies
- 시간적으로 서로 correlation이 있는 데이터를 처리하기 위해 사용
- ```The clouds are in the sky``` : are in the만 보면 sky를 유추할 수 없음. clouds까지 봐야 유추할 수 있음


### Longer-Term Dependencies
- 이전 문장만으로 글을 이해하는 것이 아닌 전체 문장까지 이해해야 현재 내용을 올바르게 이해할 수 있는 것처럼, 데이터도 동일

### LSTM : Long Short Term Memory
- 가장 기본적인 바닐라 RNN

<img src="../images/rnn002.png" height ="350">

#### 전체적 Architecture
- 공장의 컨테이너 벨트처럼 생김
- t번째 시간의 input(단어)
- 2개의 State
	- Cell State : 절대로 밖으로 빠져나가지 않는 흘러가는 State
	- Hidden State : 이전 출력
- 3개의 Gate
	- Forget Gate 
	- Input Gate
	- Output Gate
- 사용할 때는 input, output, cell state를 초기화만 잘해주면 tensorflow에서 알아서 잘해줌

<img src="../images/rnn003.png" height="350">


### Forget Gate
- 입력 : 이전 output과 현재 input
- 출력 : Cell state로 가는 어떤 값
	- activation : sigmoid(값이 0~1)
- Decide what information we're going to throw away from the cell state(cell state 값 중 어떤 것을 버릴지 결정)

### Input Gate
- 입력 : 이전 출력과 현재 입력
- 출력 
	- i_t : C_t를 얼마나 반영할지
	- C_t = tanh -1~1
- Decide what net information we're going to store in the cell state

### Update(cell state)
- Update, scled by how much we decide to update
- input_gate\*curr_state + forget_gate\*prev_state

### Output Gate
- cell state를 어떻게 밖으로 빼낼지
- 얼마나 빼낼지는 o_t가 설정

### 목적 
- 현재 입력과 이전 출력을 가지고 cell state에 어떻게 값을 집어넣고, cell state에 있는 값을 어떻게 밖으로 빼낼지를 결정










